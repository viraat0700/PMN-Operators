apiVersion: v1
data:
  cloud_alert_rules.yml: |
    groups:
    - name: cloud
      rules:

    # Disk Usage
      - alert: disk usage on cloud instance exceeds threshold
        expr: disk_used / disk_total > 0.88
        for: 5m
        labels:
          severity: critical
          magma_alert_type: cloud
          networkID: orc8r
        annotations:
          description: "Disk usage on cloud instance exceeds threshold. See ods chart for detail."
          recovery: "No recovery steps configured currently"

    # Rest 5xx Alerts
      - alert: REST API 5xx responses
        expr: rate(response_status{code=~"5.*"}[1m]) > 0
        for: 5m
        labels:
          severity: major
          magma_alert_type: cloud
          networkID: orc8r
        annotations:
          description: "Obsidian recorded a 5XX response."
          recovery: "No recovery steps configured currently"

    # Failed Cloud Service
      - alert: Failed Cloud Service
        expr: get_metrics_status{serviceName!="METRICSD"} < 1
        for: 7m
        labels:
          severity: critical
          magma_alert_type: cloud
          networkID: orc8r
        annotations:
          description: "Cloud service {{ $labels.ServiceName }} down."
          recovery: "No recovery steps configured currently"

    # Alert for metrics down to inhibit other service alerts
      - alert: Failed Metrics Service
        expr: get_metrics_status{serviceName="METRICSD"} < 1
        for: 5m
        labels:
          severity: critical
          magma_alert_type: cloud
          networkID: orc8r
        annotations:
          description: "Cloud service {{ $labels.ServiceName }} down."
          recovery: "No recovery steps configured currently"
  gateway_alert_rules.yml: |
    groups:
      - name: gateways
        rules:

        - alert: Gateway Memory Usage
          expr: avg_over_time(virtual_memory_percent[5m]) > 90
          for: 5m
          labels:
            severity: major
            magma_alert_type: gateway
            networkID: orc8r
            originatingNetwork: "{{ $labels.networkID }}"
          annotations:
            description: "Gateway {{ $labels.gatewayID }} memory usage is too high at 90% for over 5 minutes on network {{ $labels.networkID }}."
            recovery: "No recovery steps configured currently."

        - alert: Multiple gateways are failing to check in
          expr: sum(gateway_checkin_status) / count(gateway_checkin_status) <= 0.5
          for: 7m
          labels:
            severity: major
            magma_alert_type: gateway
            networkID: orc8r
            originatingNetwork: "{{ $labels.networkID }}"
          annotations:
            description: "At least 50% of gateways have not checked in the last 7 minutes!"
            recovery: >
              This many checkins failing likely means that there is a major crash
              in gateway code or there is a certificate/nginx issue. First see if
              you can ssh into any of the boxes and check syslog to see if it's
              able to contact the cloud.

        - alert: Gateway service down
          expr: process_uptime_seconds > 120 and service_metrics_collected < 1
          for: 7m
          labels:
            severity: major
            magma_alert_type: gateway
            networkID: orc8r
            originatingNetwork: "{{ $labels.networkID }}"
          annotations:
            description: "{{ $labels.service }} has been down on gateway {{ $labels.gatewayID }} for at least 7 minutes."
            recovery: "SSH into gateway and inspect service. Manually restart if necessary."

        - alert: Unattended Upgrades active
          expr: unattended_upgrade_status > 0
          for: 5m
          labels:
            severity: critical
            magma_alert_type: gateway
            networkID: orc8r
            originatingNetwork: "{{ $labels.networkID }}"
          annotations:
            description: "Unattended upgrades can update kernel in gateway {{ $labels.gatewayID }} on network {{ $labels.networkID }}"
            recovery: >
              If Unattended Upgrades package is active this means the gateway might
              automatically upgrade the kernel to an unsupported version. The best
              remedy is to SSH into the gateway and remove unattended upgrades
              package using the command
              `sudo apt-get purge --auto-remove unattended-upgrades`. We should
              also check how this package was downloaded in
              /var/log/apt/history.log.

        - alert: Unexpected service restart
          expr: rate(unexpected_service_restarts[1m]) > 0.1
          for: 15m
          labels:
            severity: major
            magma_alert_type: gateway
            networkID: orc8r
          annotations:
            description: "Unexpected service restart in gateway {{ $labels.gatewayID }} on network {{ $labels.networkID }}"
            recovery: "Check /var/log/syslog in the gateway for the root cause."
  metrics_alert_rules.yml: |
    groups:
      - name: metrics
        rules:
        - alert: Target down
          expr: up == 0
          labels:
            severity: major
            network_id: internal
            magma_alert_type: metrics
            networkID: orc8r
          annotations:
            summary: "Instance {{ $labels.instance }} - target is down"

        - alert: Prometheus Cache utilization high
          expr: cache_size / cache_limit > 0.7
          labels:
            severity: major
            network_id: internal
            magma_alert_type: metrics
            networkID: orc8r
          annotations:
            description: "Prometheus cache is running out of space"
kind: ConfigMap
metadata:
  annotations:
    meta.helm.sh/release-name: orc8r
    meta.helm.sh/release-namespace: pmn
  creationTimestamp: "2024-01-05T10:33:55Z"
  labels:
    app.kubernetes.io/managed-by: Helm
  name: orc8r-alert-rules
  namespace: pmn
  resourceVersion: "54692312"
  uid: b4d1a638-e867-49f5-bd18-93f9d09746d1